{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import keras_tuner as kt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.regularizers import L2\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.initializers import GlorotNormal, GlorotUniform, RandomNormal, RandomUniform, HeNormal, HeUniform\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy as bce_loss\n",
    "from tensorflow.keras.metrics import (BinaryAccuracy, \n",
    "    Precision,\n",
    "    Recall,\n",
    "    AUC,\n",
    "    BinaryCrossentropy as bce_metric, \n",
    ")\n",
    "from metrics.custom_metrics import f1_m\n",
    "\n",
    "from utilities.data_preprocessors import (build_results,\n",
    "    load_meta_data,\n",
    "    read_item_index_to_entity_id_file, \n",
    "    convert_rating, \n",
    "    convert_kg\n",
    ")\n",
    "\n",
    "from utilities.data_visualizers import view_vars, train_cross_results_v2\n",
    "from utilities.data_loaders import load_data_splits\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load user-item rating data splits and meta data\n",
    "meta_data = load_meta_data(f'./data/juris-600k/juris_600k_train_meta.json')\n",
    "n_users, n_items = meta_data['n_users'], meta_data['n_items']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "    hp_activation = hp.Choice('activation', values=['relu', 'tanh'])\n",
    "\n",
    "    # the drop probability values, instead of keep probability\n",
    "    hp_dropout = hp.Choice('dropout', values=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "\n",
    "    # learning rate alpha\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1.2, 0.03, 0.01, 0.0075, 0.003, 0.001, 0.0003, 0.0001])\n",
    "\n",
    "    # regularization value lambda\n",
    "    hp_lambda = hp.Choice('lambda', values=[10.0, 5.0, 1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.25, 0.125, 0.01,])\n",
    "\n",
    "    \n",
    "    \n",
    "    # since length of user_id is only a scalar. Input would be None, 1 or m x 1\n",
    "    user_id_input = tf.keras.Input(shape=(1,), dtype=tf.int64, name='user_id')\n",
    "    item_id_input = tf.keras.Input(shape=(1,), dtype=tf.int64, name='item_id')\n",
    "\n",
    "    # user and item embedding layer\n",
    "    user_emb_layer = tf.keras.layers.Embedding(n_users, 8, embeddings_regularizer=L2(hp_lambda), name='user_embedding')\n",
    "    item_emb_layer = tf.keras.layers.Embedding(n_items, 8, embeddings_regularizer=L2(hp_lambda), name='item_embedding')\n",
    "\n",
    "    # bias vector embedding layer\n",
    "    user_emb_bias_layer = tf.keras.layers.Embedding(n_users, 1, embeddings_initializer='zeros', name='user_embedding_bias')\n",
    "    item_emb_bias_layer = tf.keras.layers.Embedding(n_items, 1, embeddings_initializer='zeros', name='item_embedding_bias')\n",
    "\n",
    "    # initialize dot product layer and add layer for\n",
    "    # embedding vectors and bias scalars respectively\n",
    "    dot_layer = tf.keras.layers.Dot(axes=(2, 1))\n",
    "    add_layer = tf.keras.layers.Add()\n",
    "\n",
    "    # initialize flatten layer to flatten sum of the dot product\n",
    "    # of user_emb & item_emb, user_emb_bias, and  item_emb_bias\n",
    "    flatten_fact_matrix_layer = tf.keras.layers.Flatten()\n",
    "    \n",
    "    # initialize concat layer as input to DNN\n",
    "    concat_layer = tf.keras.layers.Concatenate(axis=2)\n",
    "    flatten_concat_emb_layer = tf.keras.layers.Flatten()\n",
    "\n",
    "    # initialize dense and activation layers of DNN\n",
    "    dense_layers = []\n",
    "    act_layers = []\n",
    "    dropout_layers = []\n",
    "\n",
    "    layers_dims = [16, 16, 16]\n",
    "    for layer_dim in layers_dims:\n",
    "        dense_layers.append(tf.keras.layers.Dense(units=layer_dim, kernel_regularizer=L2(hp_lambda)))\n",
    "        act_layers.append(tf.keras.layers.Activation(activation=hp_activation))\n",
    "\n",
    "        # drop 1 - keep_prob percent of the neurons e.g. keep_prob\n",
    "        # is 0.2 so drop 1 - 0.2 or 0.8/80% of the neurons at each \n",
    "        # activation layer\n",
    "        dropout_layers.append(tf.keras.layers.Dropout(rate=hp_dropout))\n",
    "\n",
    "    # initialize last layer of DNN to dense with no activation\n",
    "    last_dense_layer = tf.keras.layers.Dense(units=1, activation='linear', kernel_regularizer=L2(hp_lambda))\n",
    "\n",
    "    add_layer = tf.keras.layers.Add()\n",
    "\n",
    "    # output layer will just be a sigmoid activation layer\n",
    "    out_layer = tf.keras.layers.Activation(activation=tf.nn.sigmoid)\n",
    "\n",
    "\n",
    "\n",
    "    # forward pass\n",
    "    user_emb = user_emb_layer(user_id_input)\n",
    "    item_emb = item_emb_layer(item_id_input)\n",
    "\n",
    "    user_emb_bias = user_emb_bias_layer(user_id_input)\n",
    "    item_emb_bias = item_emb_bias_layer(item_id_input)\n",
    "\n",
    "    # calculate the dot product of the user_emb and item_emb vectors\n",
    "    user_item_dot = dot_layer([user_emb, tf.transpose(item_emb, perm=[0, 2, 1])])\n",
    "    fact_matrix = add_layer([user_item_dot, user_emb_bias, item_emb_bias])\n",
    "    fact_matrix_flat = flatten_fact_matrix_layer(fact_matrix)\n",
    "\n",
    "    # concatenate the user_emb and item_emb vectors\n",
    "    # then feed to fully connected deep neural net\n",
    "    A = concat_layer([user_emb, item_emb])\n",
    "    flat_A = flatten_concat_emb_layer(A)\n",
    "\n",
    "    for l in range(len(layers_dims)):\n",
    "        Z = dense_layers[l](flat_A)\n",
    "        flat_A = act_layers[l](Z)\n",
    "        flat_A = dropout_layers[l](flat_A)\n",
    "\n",
    "    A_last = last_dense_layer(flat_A)\n",
    "\n",
    "    # add the output to the flattened factorized matrix\n",
    "    sum_ = add_layer([A_last, fact_matrix_flat])\n",
    "\n",
    "    # pass the sum of last dense layer and the flattened \n",
    "    # factorized matrix to a sigmoid activation function\n",
    "    out = out_layer(sum_)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[user_id_input, item_id_input], outputs=out)\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=hp_learning_rate),\n",
    "        loss=bce_loss(),\n",
    "        metrics=[[bce_metric(), BinaryAccuracy(), Precision(), Recall(), AUC(), f1_m]]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from tuned_models\\model\\tuner0.json\n"
     ]
    }
   ],
   "source": [
    "# define tuner\n",
    "tuner = kt.Hyperband(\n",
    "    model_builder, \n",
    "    objective=kt.Objective('val_f1_m', 'max'), \n",
    "    max_epochs=100,\n",
    "    factor=3,\n",
    "    directory='tuned_models',\n",
    "    project_name='model'\n",
    ")\n",
    "\n",
    "# if cross validation loss does not improve after 10 \n",
    "# consecutive epochs we stop training our modelearly\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_f1_m', patience=30, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    185199\n",
      "1    184625\n",
      "Name: interaction, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_data, cross_data, test_data = load_data_splits('juris-600k', f'./data/juris-600k')\n",
    "print(train_data['interaction'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 238 Complete [00h 00m 23s]\n",
      "val_f1_m: 0.48639631271362305\n",
      "\n",
      "Best val_f1_m So Far: 0.6564918756484985\n",
      "Total elapsed time: 00h 38m 49s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "# fit model to data\n",
    "tuner.search(\n",
    "    [train_data['user_id'], train_data['item_id']],\n",
    "    train_data['interaction'],\n",
    "    batch_size=8192,\n",
    "    epochs=100,\n",
    "    validation_data=([cross_data['user_id'], cross_data['item_id']], cross_data['interaction']),\n",
    "    callbacks=[stop_early]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params = tuner.get_best_hyperparameters()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_names = ['activation', 'learning_rate', 'lambda', 'dropout']\n",
    "best_hyper_params = {}\n",
    "\n",
    "for hp in hp_names:\n",
    "    best_hyper_param = hyper_params.get(hp)\n",
    "\n",
    "    if hp not in best_hyper_params:\n",
    "        best_hyper_params[hp] = best_hyper_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'relu', 'learning_rate': 0.0003, 'lambda': 0.9, 'dropout': 0.9}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hyper_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('./results/best_hyper_params.json', 'w') as out_file:\n",
    "    json.dump(best_hyper_params, out_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phil-jurisprudence-recsys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
